# reference_utils.py

import pandas as pd
import numpy as np
from rdkit import Chem
from rdkit.Chem import rdMolDescriptors
from rdkit import DataStructs
from numpy.linalg import norm

def load_reference_database_with_smiles(path):
    """
    Load and preprocess the reference database for 'with_smiles' input.
    """
    final_mol2vec = pd.read_pickle(path)

    # Calculate Precursormz and Molecular_Formula
    def calculate_molecular_formula(mol):
        formula = Chem.rdMolDescriptors.CalcMolFormula(mol)
        return formula

    g_sm = final_mol2vec['smile'].tolist()

    pr_mass = []
    mol_form_mv = []
    for i in range(len(g_sm)):
        mol = Chem.MolFromSmiles(g_sm[i])
        if mol is None:
            pr_mass.append(None)
            mol_form_mv.append(None)
            continue
        m_f = calculate_molecular_formula(mol)
        mol_weight = rdMolDescriptors.CalcExactMolWt(mol)
        proton_mass = 1.007276  # mass of a proton in Da
        precursor_mass_positive = mol_weight + proton_mass
        pr_mass.append(round(precursor_mass_positive, 3))
        mol_form_mv.append(m_f)

    final_mol2vec['Precursormz'] = pr_mass
    final_mol2vec['Molecular_Formula'] = mol_form_mv
    final_mol2vec.dropna(subset=['Precursormz', 'Molecular_Formula'], inplace=True)
    final_mol2vec.reset_index(drop=True, inplace=True)

    return final_mol2vec

def load_reference_database_without_smiles(path):
    """
    Load and preprocess the reference database for 'without_smiles' input.
    """
    final_mol2vec = pd.read_pickle(path)
    # If necessary, include any required preprocessing steps similar to 'with_smiles'
    # For example, calculating 'Precursormz' if not already present
    if 'Precursormz' not in final_mol2vec.columns:
        g_sm = final_mol2vec['smile'].tolist()
        pr_mass = []
        for i in range(len(g_sm)):
            mol = Chem.MolFromSmiles(g_sm[i])
            if mol is None:
                pr_mass.append(None)
                continue
            mol_weight = rdMolDescriptors.CalcExactMolWt(mol)
            proton_mass = 1.007276  # mass of a proton in Da
            precursor_mass_positive = mol_weight + proton_mass
            pr_mass.append(round(precursor_mass_positive, 3))
        final_mol2vec['Precursormz'] = pr_mass
        final_mol2vec.dropna(subset=['Precursormz'], inplace=True)
        final_mol2vec.reset_index(drop=True, inplace=True)
    return final_mol2vec


def match_predictions_to_reference_with_smiles(prediction_df, reference_df, top_n, input_file_type):
    data_test = prediction_df.copy()
    data_test.reset_index(drop=True, inplace=True)

    # Truncate masses in data_test to three decimal places without rounding
    ls_saam = []
    for i in data_test.Precursor:
        truncated_mass = float(int(i * 1000) / 1000.0)
        ls_saam.append(truncated_mass)
    data_test['update_mass'] = ls_saam

    # Filter data_test where update_mass is in reference_df['Precursormz']
    final_test_1 = data_test[data_test['update_mass'].isin(reference_df['Precursormz'])].copy()

    up_mass = final_test_1['Precursor'].tolist()
    new_mass = []
    for i in up_mass:
        new_mass.append(float(int(i * 1000) / 1000.0))

    final_test_1['new_mass'] = new_mass

    check_test = final_test_1[final_test_1['new_mass'].isin(reference_df['Precursormz'])].copy()
    del check_test['update_mass']
    check_test = check_test.rename(columns={'new_mass': 'update_mass'})

    check_2 = data_test[~data_test['update_mass'].isin(final_test_1['update_mass'])].copy()
    final_test_up = pd.concat([check_2, check_test], ignore_index=True)

    ls_1 = final_test_up['tree_out'].tolist()

    pd.options.mode.chained_assignment = None

    final_smile = []
    final_scan_number = []
    final_EU = []
    final_Tanimoto = []

    for i in range(len(final_test_up)):
        new_df = reference_df[reference_df['Precursormz'] == final_test_up['update_mass'].iloc[i]].copy()
        new_df.reset_index(drop=True, inplace=True)
        ls_eu = []
        ls_tani = []
        for j in range(len(new_df)):
            array2 = np.array(new_df['up_molvec'].iloc[j])
            # Calculate the cosine similarity
            cosine_similarity = np.dot(ls_1[i], array2) / (norm(ls_1[i]) * norm(array2))
            ls_eu.append(cosine_similarity.item())

            # Calculate Tanimoto similarity
            ground_smile = final_test_up['smile'].iloc[i]
            candidate_smile = new_df['smile'].iloc[j]
            mol1 = Chem.MolFromSmiles(ground_smile[0])
            mol2 = Chem.MolFromSmiles(candidate_smile)
            if mol1 and mol2:
                fp1 = Chem.RDKFingerprint(mol1)
                fp2 = Chem.RDKFingerprint(mol2)
                tanimoto = DataStructs.TanimotoSimilarity(fp1, fp2)
            else:
                tanimoto = np.nan
            ls_tani.append(tanimoto)

        new_df['cosine'] = ls_eu
        new_df['Tanimoto'] = ls_tani
        # Assign Unique_ID to all rows (Unique_ID is now the SMILES string)
        new_df['Unique_ID'] = [final_test_up['Unique_ID'].iloc[i]] * len(new_df)
        new_df.sort_values('cosine', ascending=False, inplace=True)

        if len(new_df) > 0:
            top_smiles = []
            top_euclidean_distances = []
            top_tanimoto = []
            for k in range(min(len(new_df), top_n)):
                top_smiles.append(new_df['smile'].iloc[k])
                top_euclidean_distances.append(new_df['cosine'].iloc[k])
                top_tanimoto.append(new_df['Tanimoto'].iloc[k])

            final_smile.append(top_smiles)
            final_EU.append(top_euclidean_distances)
            final_scan_number.append(new_df['Unique_ID'].iloc[0])
            final_Tanimoto.append(top_tanimoto)

    result_df = pd.DataFrame({'Unique_ID': final_scan_number,
                              'Top smile': final_smile,
                              'Top Min_cosine': final_EU,
                              'Top Tanimoto': final_Tanimoto})

    # Prepare final DataFrame with top candidates
    # Generate column names based on top_n
    new_EU = [f'Top_{i+1}_cosine' for i in range(top_n)]
    new_smile = [f'Top_{i+1}_SMILE' for i in range(top_n)]
    new_tanimoto = [f'Top_{i+1}_Tanimoto' for i in range(top_n)]
    new_columns = new_EU + new_smile + new_tanimoto

    for col_name in new_columns:
        result_df[col_name] = "NA"

    for i in range(len(result_df)):
        ls_smile = result_df['Top smile'].iloc[i]
        ls_Eu = result_df['Top Min_cosine'].iloc[i]
        ls_tanimoto = result_df['Top Tanimoto'].iloc[i]
        # Add new columns in a loop
        for col_name_1, col_value_1 in zip(new_EU, ls_Eu):
            result_df.at[i, col_name_1] = col_value_1

        for col_name_2, col_value_2 in zip(new_smile, ls_smile):
            result_df.at[i, col_name_2] = col_value_2

        for col_name_3, col_value_3 in zip(new_tanimoto, ls_tanimoto):
            result_df.at[i, col_name_3] = col_value_3

    result_df.drop(['Top smile', 'Top Min_cosine', 'Top Tanimoto'], axis=1, inplace=True)
    result_df.reset_index(drop=True, inplace=True)

    return result_df




def match_predictions_to_reference_without_smiles(prediction_df, reference_df, top_n, input_file_type):
    """
    Match predictions to reference database and extract top candidates for 'without_smiles' input.
    """
    data_test = prediction_df.copy()
    data_test.reset_index(drop=True, inplace=True)

    # Truncate masses in data_test to three decimal places without rounding
    ls_saam = []
    for i in data_test.Precursor:
        truncated_mass = float(int(i * 1000) / 1000.0)
        ls_saam.append(truncated_mass)

    data_test['update_mass'] = ls_saam

    # Filter data_test where update_mass is in reference_df['Precursormz']
    final_test_1 = data_test[data_test['update_mass'].isin(reference_df['Precursormz'])].copy()

    up_mass = final_test_1['Precursor'].tolist()
    new_mass = []
    for i in range(len(final_test_1)):
        up_m = up_mass[i]
        new_mass.append(round(up_m, 3))

    final_test_1['new_mass'] = new_mass

    check_test = final_test_1[final_test_1['new_mass'].isin(reference_df['Precursormz'])].copy()
    del check_test['update_mass']
    check_test = check_test.rename(columns={'new_mass': 'update_mass'})

    check_2 = data_test[~data_test['update_mass'].isin(final_test_1['update_mass'])].copy()
    final_test_up = pd.concat([check_2, check_test], ignore_index=True)

    ls_1 = final_test_up['tree_out'].tolist()

    pd.options.mode.chained_assignment = None

    final_smile = []
    final_scan_number = []
    final_EU = []
    final_inchikey = []

    for i in range(len(final_test_up)):
        new_df = reference_df[reference_df['Precursormz'] == final_test_up['update_mass'].iloc[i]].copy()
        new_df.reset_index(drop=True, inplace=True)
        ls_eu = []
        for j in range(len(new_df)):
            array2 = np.array(new_df['up_molvec'].iloc[j])
            # Calculate the cosine similarity
            cosine_similarity = np.dot(ls_1[i], array2) / (norm(ls_1[i]) * norm(array2))
            ls_eu.append(cosine_similarity.item())

        new_df['cosine'] = ls_eu
        # Assign Unique_ID to all rows
        new_df['Unique_ID'] = [final_test_up['Unique_ID'].iloc[i]] * len(new_df)
        new_df.sort_values('cosine', ascending=False, inplace=True)

        if len(new_df) > 0:
            top_smiles = []
            top_euclidean_distances = []
            top_inchikey = []
            for k in range(min(len(new_df), top_n)):
                top_smiles.append(new_df['smile'].iloc[k])  # Assuming 'smile' exists in reference_df
                top_euclidean_distances.append(new_df['cosine'].iloc[k])
                top_inchikey.append(new_df['inchikey'].iloc[k])  # Assuming 'inchikey' exists in reference_df

            final_smile.append(top_smiles)
            final_EU.append(top_euclidean_distances)
            final_scan_number.append(new_df['Unique_ID'].iloc[0])
            final_inchikey.append(top_inchikey)

    result_df = pd.DataFrame({'Unique_ID': final_scan_number,
                              'Top smile': final_smile,
                              'Top Min_cosine': final_EU,
                              'Top InChIKey': final_inchikey})

    # Prepare final DataFrame with top candidates
    # Generate column names based on top_n
    new_EU = [f'Top_{i+1}_cosine' for i in range(top_n)]
    new_smile = [f'Top_{i+1}_SMILE' for i in range(top_n)]
    new_inchikey = [f'Top_{i+1}_InChIKey' for i in range(top_n)]
    new_columns = new_EU + new_smile + new_inchikey

    for col_name in new_columns:
        result_df[col_name] = "NA"

    for i in range(len(result_df)):
        ls_smile = result_df['Top smile'].iloc[i]
        ls_Eu = result_df['Top Min_cosine'].iloc[i]
        ls_inchikey = result_df['Top InChIKey'].iloc[i]
        # Add new columns in a loop
        for col_name_1, col_value_1 in zip(new_EU, ls_Eu):
            result_df.at[i, col_name_1] = col_value_1

        for col_name_2, col_value_2 in zip(new_smile, ls_smile):
            result_df.at[i, col_name_2] = col_value_2

        for col_name_3, col_value_3 in zip(new_inchikey, ls_inchikey):
            result_df.at[i, col_name_3] = col_value_3

    result_df.drop(['Top smile', 'Top Min_cosine', 'Top InChIKey'], axis=1, inplace=True)
    result_df.reset_index(drop=True, inplace=True)

    return result_df
